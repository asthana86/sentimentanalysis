<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.ML.PCA</name>
    </assembly>
    <members>
        <member name="T:Microsoft.ML.Runtime.PCA.RandomizedPcaTrainer">
            <summary>
            This trainer trains an approximate PCA using Randomized SVD algorithm
            Reference: https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf
            </summary>
            <remarks>
            This PCA can be made into Kernel PCA by using Random Fourier Features transform
            </remarks>
        </member>
        <member name="M:Microsoft.ML.Runtime.PCA.RandomizedPcaTrainer.PostProcess(Microsoft.ML.Runtime.Data.VBuffer{System.Single}[],System.Single[],System.Single[],System.Int32,System.Int32)">
            <summary>
            Modifies <paramref name="y"/> in place so it becomes <paramref name="y"/> * eigenvectors / eigenvalues.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.PCA.PcaPredictor">
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
        </member>
        <member name="M:Microsoft.ML.Runtime.PCA.PcaPredictor.GetEigenVectors(Microsoft.ML.Runtime.Data.VBuffer{System.Single}[]@,System.Int32@)">
            <summary>
            Copies the top eigenvectors of the covariance matrix of the training data
            into a set of buffers.
            </summary>
            <param name="vectors">A possibly reusable set of vectors, which will
            be expanded as necessary to accomodate the data.</param>
            <param name="rank">Set to the rank, which is also the logical length
            of <paramref name="vectors"/>.</param>
        </member>
        <member name="M:Microsoft.ML.Runtime.PCA.PcaPredictor.GetMean(Microsoft.ML.Runtime.Data.VBuffer{System.Single}@)">
            <summary>
            Copies the mean vector of the training data.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Runtime.Data.PcaTransform">
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
        </member>
        <member name="M:Microsoft.ML.Runtime.Data.PcaTransform.#ctor(Microsoft.ML.Runtime.IHostEnvironment,Microsoft.ML.Runtime.Data.PcaTransform.Arguments,Microsoft.ML.Runtime.Data.IDataView)">
            <summary>
            Public constructor corresponding to SignatureDataTransform.
            </summary>
        </member>
    </members>
</doc>
